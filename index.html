<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Contrastive Learning, Data-Efficient, Self-Supervised Learning">
  <meta property="og:title" content="Data-Efficient Contrastive Learning"/>
  <meta property="og:description" content="A paper on how to choose good subsets of data to train on that allow contrastive learning on the subset to match the perfomrance on the full data"/>
  <meta property="og:url" content="https://sjoshi804.github.io"/>


  <meta name="twitter:title" content="Data-Efficient Contrastive Learning (ICML 2023)">
  <meta name="twitter:description" content="Project Page (with links to paper and code) for Data-Efficient Contrastive Learning (ICML 2023)">
  <!-- Path git@github.com:sjoshi804/data-efficient-contrastive-learning.gitto banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/discard-pct.png">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="contrastive learning data-efficient multimodal learning SSL">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Data-Efficient Contrastive Learning</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Data-Efficient Contrastive Learning</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://sjoshi804.github.io" target="_blank">Siddharth Joshi</a>,</span>
                <span class="author-block">
                  <a href="http://web.cs.ucla.edu/~baharan/index.htm" target="_blank">Baharan Mirzasoleiman</a>
                </span>
                  </div>
                  <div class="is-size-5 publication-authors">
                    <span class="author-block">UCLA CS <br> <b>ICML 2023</b></span>
                  </div>
                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2302.09195" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>


                    <span class="link-block">
                      <a href="https://drive.google.com/file/d/1Q784t6-aRH_HX8aQGMOwogUWj-cH25P7/view?usp=sharing" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Poster</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/sjoshi804/sas-data-efficient-contrastive-learning" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                  <span class="link-block">
                    <a href="https://www.youtube.com/watch?v=KbEM0RROC9w&ab_channel=SiddharthJoshi" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <svg class="svg-inline--fa fa-youtube fa-w-18" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="youtube" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512" data-fa-i2svg=""><path fill="currentColor" d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg><!-- <i class="fab fa-youtube"></i> Font Awesome fontawesome.com -->
                      </span>
                      <span>Video</span>
                    </a>
                  </span>
                </span>

                <!-- ArXiv abstract Link 
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
       Visualization of the kinds of points we pick!
    </div>
  </div>
</section>
End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Self-supervised learning (SSL) learns high-quality representations from large pools of unlabeled training data. As datasets grow larger, it becomes crucial to identify the examples that contribute the most to learning such representations. This enables efficient SSL by reducing the volume of data required for learning high-quality representations. Nevertheless, quantifying the value of examples for SSL has remained an open question. In this work, we address this for the first time, by proving that examples that contribute the most to contrastive SSL are those that have the most similar augmentations to other examples, in expectation. We provide rigorous guarantees for the generalization performance of SSL on such subsets. Empirically, we discover, perhaps surprisingly, the subsets that contribute the most to SSL are those that contribute the least to supervised learning. Through extensive experiments, we show we can safely exclude 20% of examples from CIFAR100 and 40% from STL10 and TinyImageNet, without affecting downstream task performance. We also show that our subsets outperform random subsets by more than 2% on CIFAR10. We also demonstrate that these subsets are effective across contrastive SSL methods (evaluated on SimCLR, MoCo, SimSiam, BYOL).
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- End paper abstract -->
<br>
<br>
<div class="columns is-centered">
  <div class="column is-three-fifths">
    <h2 class="title is-3">Highlights</h2>
      <h3 class="title is-4">New Problem</h3>
      <hr>
      <div class="content has-text-justified">
        <p>
          Previous work has studied data-efficiency for supervised learning and shown that nearly 30% of examples can be discarded on many datasets without sacrificing accuracy. 
          However, this problem has not been studied for self-supervised learning. Our paper proposes the first empirical and theoretical method for data-efficient contrastive learning.
          We show that across various datasets (CIFAR100, STL10 and TinyImageNet), our method allows you to discard 20-40% of examples without affecting downstream classification accuracy.
        </p>
      </div>
      <img src="./static/images/discard-pct.png" class="is-centered">
      <br>
      <h3 class="title is-4">Theoretical Guarantees for Downstream Accuracy</h3>
      <hr>
      <div class="content has-text-justified">
        <p>
          We use <a href="https://arxiv.org/abs/2111.0074">[Huang et al. 2023]</a> theoretical characterization of contrastive learning using <b>alignment</b> and <b>divergence of class centers</b> to show that the subset
          we choose preserves alignment and divergence of class centers and thus preserves downstream accuracy. 
        </p>
      </div>
  </div>
</div>

<!-- Image carousel 
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <img src="static/images/carousel1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
  </div>
</div>
</div>
</section>
 End image carousel -->




<!-- Youtube video 
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <iframe src="" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
End youtube video -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{joshi2023dataefficient,
        title={Data-Efficient Contrastive Self-supervised Learning: Easy Examples Contribute the Most}, 
        author={Siddharth Joshi and Baharan Mirzasoleiman},
        year={2023},
        eprint={2302.09195},
        archivePrefix={arXiv},
        primaryClass={cs.LG}}
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


  </body>
  </html>
